{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Words2Vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIr+xI/giIN5CkKMXiA5pC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanTokManMokMTM/NLP_Training4/blob/main/Words2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gurqneDTAv9"
      },
      "source": [
        "!unzip wiki_zh_2019\n",
        "#unzip the zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt7noXba87u-"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "import json\n",
        "ALL_DATAS = []\n",
        "filesList = {\n",
        "    0:\"AA\",\n",
        "    1:\"AB\",\n",
        "    2:\"AC\",\n",
        "    4:\"AD\",\n",
        "    5:\"AE\",\n",
        "    6:\"AF\",\n",
        "    7:\"AG\",\n",
        "    8:\"AH\",    \n",
        "    9:\"AI\",\n",
        "    10:\"AJ\",\n",
        "    11:\"AK\",\n",
        "    12:\"AL\",\n",
        "    13:\"AM\"\n",
        "}\n",
        "\n",
        "for i in filesList:\n",
        "  folderPath = \"./wiki_zh/%s/\" % filesList[i]\n",
        "  files = listdir(folderPath)\n",
        "  for i in files:\n",
        "    curFilePath = folderPath + \"%s\"%i \n",
        "    print(curFilePath)\n",
        "    with open(curFilePath) as f:\n",
        "      for j in f.readlines():\n",
        "        str2Json = json.loads(j)\n",
        "        text = \"\".join(str2Json[\"text\"].strip().split())\n",
        "        ALL_DATAS.append(text)\n",
        "      f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYpudYkjGhXQ"
      },
      "source": [
        "!pip install opencc-python-reimplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCbWRwDLHXFT"
      },
      "source": [
        "#convert text \n",
        "from opencc import OpenCC\n",
        "#convert to zh-hk\n",
        "ccConvertor = OpenCC(\"s2hk\")\n",
        "transChineseList = []\n",
        "for i in ALL_DATAS:\n",
        "  transChineseList.append(ccConvertor.convert(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YdI4-yWCVTr"
      },
      "source": [
        "#store convertd data to csv\n",
        "import pandas as pd\n",
        "datas = pd.DataFrame(transChineseList)\n",
        "datas.to_csv(\"alldatas.csv\",encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOQo3P3IVizE"
      },
      "source": [
        "# cut words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2GY2jGwXJqM",
        "outputId": "05721a24-a23d-4183-c390-93b9a7ac1fa1"
      },
      "source": [
        "!pip install jieba"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (0.42.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flknmXrPXraT"
      },
      "source": [
        "#load stopwords list\n",
        "import codecs \n",
        "fp = codecs.open(\"./stopwords.dat\",\"r\",encoding=\"utf-8\")\n",
        "contents = fp.read()\n",
        "fp.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaxTlIFXXLuM"
      },
      "source": [
        "import jieba\n",
        "import jieba.analyse\n",
        "cutWordsListDatas = []\n",
        "x = 0;\n",
        "for i in transChineseList:\n",
        "  cutWordsListDatas.append(\" \".join([word for word in jieba.cut(i,cut_all=False) if word not in contents]))\n",
        "  if(x%5000 == 0):\n",
        "    print(x)\n",
        "  x+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoMbd9mepzAE"
      },
      "source": [
        "cutWordsTxt = \"cutWordsList2.txt\"\n",
        "with open(cutWordsTxt,\"w\",encoding=\"utf-8\") as fw:\n",
        "  for data in cutWordsListDatas:\n",
        "    fw.write(data)\n",
        "    fw.write(\"\\n\")\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXYdX5McsRny"
      },
      "source": [
        "# Word2vec for our text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G75K0TIIUW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117aad68-f1e8-4767-8826-4f58c5c8b452"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFhjkUNGIa1u"
      },
      "source": [
        "#store txt to array\\\n",
        "cutWordsListDatas = []\n",
        "with open(\"/content/drive/MyDrive/cutWordsList.txt\") as f:\n",
        "  for i in f.readlines():\n",
        "    cutWordsListDatas.append(i.rstrip(\"\\n\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSOaOCvsVK6"
      },
      "source": [
        "!pip install word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuovbVx6XyvM",
        "outputId": "c19d97e0-bcce-4e47-ab5d-1e101b2d1380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "import word2vec\n",
        "word2vec.word2vec(\"cutWordsList2.txt\",\"cutWordsList2.bin\",size=300,min_count=1,verbose=True,threads=8,binary=1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running command: word2vec -train cutWordsList2.txt -output cutWordsList2.bin -size 300 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 8 -iter 5 -min-count 1 -alpha 0.025 -debug 2 -binary 1 -cbow 1\n",
            "Starting training using file cutWordsList2.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-ca6c581e20d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cutWordsList2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cutWordsList2.bin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/word2vec/scripts_interface.py\u001b[0m in \u001b[0;36mword2vec\u001b[0;34m(train, output, size, window, sample, hs, negative, threads, iter_, min_count, alpha, debug, binary, cbow, save_vocab, read_vocab, verbose)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mrun_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/word2vec/scripts_interface.py\u001b[0m in \u001b[0;36mrun_cmd\u001b[0;34m(command, verbose)\u001b[0m\n\u001b[1;32m    283\u001b[0m         raise Exception(\n\u001b[1;32m    284\u001b[0m             \u001b[0;34m\"The training could not be completed (returncode=%i): %s %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         )\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The training could not be completed (returncode=-9): b'' b'tcmalloc: large alloc 6887137280 bytes == 0x55c7905e8000 @  0x7f4ec2350b6b 0x7f4ec2370379 0x55c72a77c689 0x55c72a77ce90 0x55c72a776467 0x7f4ec19aebf7 0x55c72a7768ba\\ntcmalloc: large alloc 6887137280 bytes == 0x55c92b6a2000 @  0x7f4ec2350b6b 0x7f4ec2370379 0x55c72a77c6d7 0x55c72a77ce90 0x55c72a776467 0x7f4ec19aebf7 0x55c72a7768ba\\n'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUk_kFv-ZNb6"
      },
      "source": [
        "model = word2vec.load(\"cutWordsListTest.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYKSxCn4Ze_n"
      },
      "source": [
        "temp = model.similar(\"北京\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttNihKbvaNlJ",
        "outputId": "a1324d54-bdc5-4632-bf61-dff0e951a146",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in temp[0]:\n",
        "  print(model.vocab[i],model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "上海 148\n",
            "下半年 9268\n",
            "第四 773\n",
            "教務長 17331\n",
            "院長 1704\n",
            "撤消 9341\n",
            "月間 10072\n",
            "工程局 13684\n",
            "教育部 1067\n",
            "中國國家 3638\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}