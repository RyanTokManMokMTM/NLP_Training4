{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Words2Vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsEwPRLvCjUJ25Fc+v+8J4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanTokManMokMTM/NLP_Training4/blob/main/Words2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpyNNP_lOWcd",
        "outputId": "e390ae30-550d-4c95-d03b-a23b11c8cad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gurqneDTAv9"
      },
      "source": [
        "!unzip /content/drive/MyDrive/wiki_zh_2019\n",
        "#unzip the zipfile\n",
        "!pip install opencc-python-reimplemente\n",
        "!pip install jieba\n",
        "!pip install word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B84ktA8GKxM2"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "import json\n",
        "from opencc import OpenCC\n",
        "filesList = {\n",
        "    0:\"AA\",\n",
        "    1:\"AB\",\n",
        "    2:\"AC\",\n",
        "    4:\"AD\",\n",
        "    5:\"AE\",\n",
        "    6:\"AF\",\n",
        "    7:\"AG\",\n",
        "    8:\"AH\",    \n",
        "    9:\"AI\",\n",
        "    10:\"AJ\",\n",
        "    11:\"AK\",\n",
        "    12:\"AL\",\n",
        "    13:\"AM\"\n",
        "}\n",
        "\n",
        "def loadData():\n",
        "  ALL_DATAS = []\n",
        "  for i in filesList:\n",
        "    folderPath = \"./wiki_zh/%s/\" % filesList[i]\n",
        "    files = listdir(folderPath)\n",
        "    for i in files:\n",
        "      curFilePath = folderPath + \"%s\"%i \n",
        "      print(curFilePath)\n",
        "      with open(curFilePath) as f:\n",
        "        for j in f.readlines():\n",
        "          str2Json = json.loads(j)\n",
        "          text = \"\".join(str2Json[\"text\"].strip().split())\n",
        "          ALL_DATAS.append(text)\n",
        "        f.close()\n",
        "  chineseSimpTotrad(ALL_DATAS)\n",
        "\n",
        "def chineseSimpTotrad(data):\n",
        "  ccConvertor = OpenCC(\"s2hk\")\n",
        "  transChineseList = []\n",
        "  for i in data:\n",
        "    transChineseList.append(ccConvertor.convert(i))\n",
        "  return transChineseList\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOQo3P3IVizE"
      },
      "source": [
        "# cut words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flknmXrPXraT"
      },
      "source": [
        "#load stopwords list\n",
        "import codecs\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "\n",
        "def wordCutting(wordList,stopWordlist):\n",
        "  cutWordsListDatas = []\n",
        "  for i in wordList:\n",
        "      cutWordsListDatas.append(\" \".join([word for word in jieba.cut(i,cut_all=False) if word not in stopWordlist]))\n",
        "  return cutWordsListDatas\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXYdX5McsRny"
      },
      "source": [
        "# Word2vec for our text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0I34O7OehTD"
      },
      "source": [
        "import word2vec\n",
        "\n",
        "def modelGenerate(cutWordTxt):\n",
        "  word2vec.word2vec(cutWordTxt,\"demo.bin\",size=300,verbose=True,binary=True)\n",
        "  print(\"Done....\")\n",
        "\n",
        "def loadModel(binFielPath):\n",
        "  return word2vec.load(binFielPath)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecz8xDEB0dmj",
        "outputId": "5792c0a5-165f-4baf-d300-9941d01005aa"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  #load All data and convert to traditional chinese\n",
        "  chineseWordList = loadData()\n",
        "\n",
        "  #load stop word list\n",
        "  fp = codecs.open(\"./stopwords.dat\",\"r\",encoding=\"utf-8\")\n",
        "  contents = fp.read()\n",
        "  fp.close()\n",
        "\n",
        "  # word cutting and stoplist\n",
        "  cutWordsListDatas = wordCutting(chineseWordList,contents)\n",
        "\n",
        "  # #store to a txt\n",
        "  cutWordsTxt = \"cutWordsList2.txt\"\n",
        "  with open(cutWordsTxt,\"w\",encoding = \"utf-8\") as fw:\n",
        "    for data in cutWordsListDatas:\n",
        "      fw.write(data)\n",
        "      fw.write(\"\\n\")\n",
        "\n",
        "  modelGenerate(cutWordsTxt)\n",
        "  model = loadModel(\"/content/drive/MyDrive/demo.bin\")\n",
        "  predictWord = \"鋼鐵人\"\n",
        "  datas = model.similar(predictWord,n=20)\n",
        "  word = list(datas[0])\n",
        "  score = list(datas[1])\n",
        "  for i in range(len(word)):\n",
        "      print(\"%s : %s\" % (model.vocab[word[i]],score[i]))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running command: word2vec -train cutWordsList2.txt -output demo.bin -size 300 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 12 -iter 5 -min-count 5 -alpha 0.025 -debug 2 -binary 1 -cbow 1\n",
            "Starting training using file cutWordsList2.txt\n",
            "Vocab size: 1\n",
            "Words in train file: 1\n",
            "Done....\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}