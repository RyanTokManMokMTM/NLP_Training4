{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Words2Vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOj9w031uAPugvsd5QfBcbH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanTokManMokMTM/NLP_Training4/blob/main/Words2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpyNNP_lOWcd",
        "outputId": "e390ae30-550d-4c95-d03b-a23b11c8cad5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gurqneDTAv9"
      },
      "source": [
        "!unzip /content/drive/MyDrive/wiki_zh_2019\n",
        "#unzip the zipfile\n",
        "!pip install opencc-python-reimplemente\n",
        "!pip install jieba\n",
        "!pip install word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B84ktA8GKxM2"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "import json\n",
        "from opencc import OpenCC\n",
        "filesList = {\n",
        "    0:\"AA\",\n",
        "    1:\"AB\",\n",
        "    2:\"AC\",\n",
        "    4:\"AD\",\n",
        "    5:\"AE\",\n",
        "    6:\"AF\",\n",
        "    7:\"AG\",\n",
        "    8:\"AH\",    \n",
        "    9:\"AI\",\n",
        "    10:\"AJ\",\n",
        "    11:\"AK\",\n",
        "    12:\"AL\",\n",
        "    13:\"AM\"\n",
        "}\n",
        "\n",
        "def loadData():\n",
        "  ALL_DATAS = []\n",
        "  for i in filesList:\n",
        "    folderPath = \"./wiki_zh/%s/\" % filesList[i]\n",
        "    files = listdir(folderPath)\n",
        "    for i in files:\n",
        "      curFilePath = folderPath + \"%s\"%i \n",
        "      print(curFilePath)\n",
        "      with open(curFilePath) as f:\n",
        "        for j in f.readlines():\n",
        "          str2Json = json.loads(j)\n",
        "          text = \"\".join(str2Json[\"text\"].strip().split())\n",
        "          ALL_DATAS.append(text)\n",
        "        f.close()\n",
        "  chineseSimpTotrad(ALL_DATAS)\n",
        "\n",
        "def chineseSimpTotrad(data):\n",
        "  ccConvertor = OpenCC(\"s2hk\")\n",
        "  transChineseList = []\n",
        "  for i in data:\n",
        "    transChineseList.append(ccConvertor.convert(i))\n",
        "  return transChineseList\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOQo3P3IVizE"
      },
      "source": [
        "# cut words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flknmXrPXraT"
      },
      "source": [
        "#load stopwords list\n",
        "import codecs\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "\n",
        "def wordCutting(wordList,stopWordlist):\n",
        "  cutWordsListDatas = []\n",
        "  for i in wordList:\n",
        "      cutWordsListDatas.append(\" \".join([word for word in jieba.cut(i,cut_all=False) if word not in stopWordlist]))\n",
        "  return cutWordsListDatas\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXYdX5McsRny"
      },
      "source": [
        "# Word2vec for our text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0I34O7OehTD"
      },
      "source": [
        "import word2vec\n",
        "\n",
        "def modelGenerate(cutWordTxt):\n",
        "  word2vec.word2vec(cutWordTxt,\"demo.bin\",size=300,verbose=True,binary=True)\n",
        "  print(\"Done....\")\n",
        "\n",
        "def loadModel(binFielPath):\n",
        "  return word2vec.load(binFielPath)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecz8xDEB0dmj",
        "outputId": "f4253682-c646-4d24-8ca7-dc7c12d70926"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  #load All data and convert to traditional chinese\n",
        "  # chineseWordList = loadData()\n",
        "\n",
        "  # #load stop word list\n",
        "  # fp = codecs.open(\"./stopwords.dat\",\"r\",encoding=\"utf-8\")\n",
        "  # contents = fp.read()\n",
        "  # fp.close()\n",
        "\n",
        "  # # word cutting and stoplist\n",
        "  # cutWordsListDatas = wordCutting(chineseWordList,contents)\n",
        "\n",
        "  # # #store to a txt\n",
        "  # cutWordsTxt = \"cutWordsList2.txt\"\n",
        "  # with open(cutWordsTxt,\"w\",encoding = \"utf-8\") as fw:\n",
        "  #   for data in cutWordsListDatas:\n",
        "  #     fw.write(data)\n",
        "  #     fw.write(\"\\n\")\n",
        "\n",
        "  # modelGenerate(cutWordsTxt)\n",
        "  model = loadModel(\"/content/drive/MyDrive/demo.bin\")\n",
        "  predictWord = \"鋼鐵人\"\n",
        "  datas = model.similar(predictWord,n=20)\n",
        "  word = list(datas[0])\n",
        "  score = list(datas[1])\n",
        "  for i in range(len(word)):\n",
        "      print(\"%s : %s\" % (model.vocab[word[i]],score[i]))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "蜘蛛人 : 0.7715826053998335\n",
            "水行俠 : 0.737633098643578\n",
            "仇者 : 0.7172986328173914\n",
            "鋼鐵俠 : 0.7097133364879444\n",
            "夜魔俠 : 0.6916923892783963\n",
            "死侍 : 0.6905602600709029\n",
            "漫威 : 0.6771899330465423\n",
            "浩克 : 0.6771353180448477\n",
            "閃電俠 : 0.6745832078566145\n",
            "綠燈俠 : 0.6715115775461686\n",
            "特攻隊 : 0.6641374024557118\n",
            "戰甲 : 0.6584346089582496\n",
            "鐵拳俠 : 0.654428165528822\n",
            "綠箭俠 : 0.6527442821540076\n",
            "女超人 : 0.648465130172418\n",
            "魔俠 : 0.6450306426433308\n",
            "超人 : 0.6397801118649373\n",
            "魔龍 : 0.6333124260608268\n",
            "前傳 : 0.6330088338004289\n",
            "美國隊 : 0.629012049371652\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}